# -*- coding: utf-8 -*-
"""FINAL CLASSIFICATION DENSENET.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14zdUsB-p0Hg41PsHMjfgRWMJF5OwJ_0R
"""

from google.colab import drive
drive.mount('/content/drive')

import os  # For directory and file operations
import numpy as np  # For numerical operations and handling image arrays
import random  # For generating random values for augmentation
from PIL import Image, ImageEnhance  # For image processing and enhancement
from tensorflow.keras.preprocessing.image import load_img  # For loading images
from tensorflow.keras.models import Sequential  # For building the model
from tensorflow.keras.layers import Input, Flatten, Dropout, Dense  # For model layers
from tensorflow.keras.optimizers import Adam  # For optimizer
from tensorflow.keras.applications import VGG16  # For using VGG16 model
from sklearn.utils import shuffle  # For shuffling the data

training_dir = "/content/drive/My Drive/brain_tumor/Training" # Replace with your actual path
training_files = os.listdir(training_dir)

print("Files in the training directory:")
for file in training_files:
    print(file)
    testing_dir = "/content/drive/My Drive/brain_tumor/Testing"
testing_files = os.listdir(training_dir)

print("Files in the testing directory:")
for file in testing_files:
    print(file)

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import random
from PIL import Image, ImageEnhance
from tensorflow.keras.preprocessing.image import load_img
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.applications.densenet import preprocess_input
from sklearn.utils import shuffle
import matplotlib.pyplot as plt

# Define directories
training_dir = "/content/drive/My Drive/brain_tumor/Training"
testing_dir = "/content/drive/My Drive/brain_tumor/Testing"

# Verify training directory
training_files = os.listdir(training_dir)
print("Training Classes:", training_files)

# Verify testing directory
testing_files = os.listdir(testing_dir)
print("Testing Classes:", testing_files)

def load_images_with_augmentation(directory, img_size=(224, 224), augment_factor=2):
    images = []
    labels = []
    class_names = sorted(os.listdir(directory))

    for class_idx, class_name in enumerate(class_names):
        class_dir = os.path.join(directory, class_name)
        if not os.path.isdir(class_dir):
            continue

        image_files = [f for f in os.listdir(class_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]

        for image_file in image_files:
            image_path = os.path.join(class_dir, image_file)
            image = load_img(image_path, target_size=img_size)
            image_array = np.array(image)
            image_array = preprocess_input(image_array)  # DenseNet-specific preprocessing
            images.append(image_array)
            labels.append(class_idx)

            # Augmentations
            for _ in range(augment_factor):
                # Random rotation
                angle = random.uniform(-20, 20)
                rotated = image.rotate(angle)
                # Random flip
                if random.random() > 0.5:
                    rotated = rotated.transpose(Image.FLIP_LEFT_RIGHT)
                # Random brightness
                enhancer = ImageEnhance.Brightness(rotated)
                factor = random.uniform(0.8, 1.2)
                enhanced = enhancer.enhance(factor)
                # Process augmented image
                augmented_array = np.array(enhanced)
                augmented_array = preprocess_input(augmented_array)
                images.append(augmented_array)
                labels.append(class_idx)

    return np.array(images), np.array(labels)

# Load data
X_train, y_train = load_images_with_augmentation(training_dir, augment_factor=2)
X_test, y_test = load_images_with_augmentation(testing_dir, augment_factor=0)  # No augmentation for testing

# Shuffle training data
X_train, y_train = shuffle(X_train, y_train, random_state=42)

# Initialize base model
base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False  # Freeze base layers

# Build custom model
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(len(os.listdir(training_dir)), activation='softmax')  # Dynamic class count
])

model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

# Initialize base model
base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False  # Freeze base layers

# Build custom model
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(len(os.listdir(training_dir)), activation='softmax')  # Dynamic class count
])

model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

from tensorflow.keras.applications import DenseNet201
from tensorflow.keras.layers import BatchNormalization

# Use larger DenseNet variant
base_model = DenseNet201(
    weights='imagenet',
    include_top=False,
    input_shape=(224, 224, 3)
)

#Get class names from the training directory
class_names = sorted(os.listdir(training_dir))

# Unfreeze top layers
base_model.trainable = True
for layer in base_model.layers[:-20]:
    layer.trainable = False

model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(512, activation='relu', kernel_regularizer='l2'),
    BatchNormalization(),
    Dropout(0.6),
    Dense(256, activation='relu', kernel_regularizer='l2'),
    BatchNormalization(),
    Dropout(0.5),
    Dense(len(class_names), activation='softmax')
])

from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping

optimizer = Adam(learning_rate=1e-4)  # Lower initial LR

callbacks = [
    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3),
    EarlyStopping(monitor='val_accuracy', patience=8, restore_best_weights=True)
]

model.compile(optimizer=optimizer,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=25,
    batch_size=32
)

# Plot accuracy
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training vs Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training vs Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

# Step 1: Calculate final accuracy
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
print(f"\nTest Accuracy: {test_acc:.4f}")
print(f"Test Loss: {test_loss:.4f}")

# Step 2: Generate predictions
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

# Step 3: Confusion Matrix
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report

# Get class names
class_names = sorted(os.listdir(training_dir))

# Generate confusion matrix
cm = confusion_matrix(y_test, y_pred_classes)

# Plot confusion matrix
plt.figure(figsize=(10, 8))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical')
plt.title('Confusion Matrix')
plt.show()

# Step 4: Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred_classes, target_names=class_names, digits=4))

# Calculate standard deviation of training and validation accuracy
train_acc_std = np.std(history.history['accuracy'])
val_acc_std = np.std(history.history['val_accuracy'])

print(f"\nStandard Deviation of Training Accuracy: {train_acc_std:.4f}")
print(f"Standard Deviation of Validation Accuracy: {val_acc_std:.4f}")

from sklearn.model_selection import KFold

# Cross-validation configuration
k = 5  # Number of folds
kfold = KFold(n_splits=k, shuffle=True, random_state=42)
cv_accuracies = []
cv_losses = []

# Cross-validation loop
for fold, (train_ids, val_ids) in enumerate(kfold.split(X_train, y_train)):
    print(f"\nTraining fold {fold + 1}/{k}")

    # Re-initialize model with fresh weights for each fold
    base_model = DenseNet201(
        weights='imagenet',
        include_top=False,
        input_shape=(224, 224, 3)
    )
    # Freeze/unfreeze layers
    base_model.trainable = True
    for layer in base_model.layers[:-20]:
        layer.trainable = False

    # Rebuild model architecture
    model = Sequential([
        base_model,
        GlobalAveragePooling2D(),
        Dense(512, activation='relu', kernel_regularizer='l2'),
        BatchNormalization(),
        Dropout(0.6),
        Dense(256, activation='relu', kernel_regularizer='l2'),
        BatchNormalization(),
        Dropout(0.5),
        Dense(len(class_names), activation='softmax')
    ])

    # Recompile model
    model.compile(
        optimizer=Adam(learning_rate=1e-4),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    # Define fresh callbacks for each fold
    callbacks = [
        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3),
        EarlyStopping(monitor='val_accuracy', patience=8, restore_best_weights=True)
    ]

    # Train on current fold
    history = model.fit(
        X_train[train_ids], y_train[train_ids],
        validation_data=(X_train[val_ids], y_train[val_ids]),
        epochs=25,
        batch_size=32,
        callbacks=callbacks,
        verbose=1
    )

    # Store validation results
    val_loss, val_acc = model.evaluate(X_train[val_ids], y_train[val_ids], verbose=0)
    cv_accuracies.append(val_acc)
    cv_losses.append(val_loss)
    print(f"Fold {fold + 1} Validation Accuracy: {val_acc:.4f}")

# Cross-validation results summary
print("\nCross-Validation Results:")
print(f"{k}-Fold Average Validation Accuracy: {np.mean(cv_accuracies):.4f} (±{np.std(cv_accuracies):.4f})")
print(f"{k}-Fold Average Validation Loss: {np.mean(cv_losses):.4f} (±{np.std(cv_losses):.4f})")

# Generate predictions
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

# Confusion matrix
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report

cm = confusion_matrix(y_test, y_pred_classes)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)

plt.figure(figsize=(10, 8))
disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical')
plt.title('Confusion Matrix (Test Set)')
plt.show()

# Classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred_classes, target_names=class_names, digits=4))

plt.figure(figsize=(12, 4))

# Accuracy plot
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title('Training vs Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Loss plot
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Training vs Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

model.save("brain_tumor_classifier.h5")

pip install gradio

import gradio as gr
from tensorflow.keras.models import load_model
from tensorflow.keras.applications.densenet import preprocess_input
from tensorflow.keras.preprocessing.image import img_to_array
from PIL import Image
import numpy as np
import os

# Load trained model
model = load_model("brain_tumor_classifier.h5")

# Class labels
class_names = sorted(os.listdir("/content/drive/My Drive/brain_tumor/Training"))

# Prediction function
def predict(image):
    image = image.resize((224, 224))
    image_array = img_to_array(image)
    image_array = np.expand_dims(image_array, axis=0)
    image_array = preprocess_input(image_array)

    predictions = model.predict(image_array)[0]
    predicted_class = class_names[np.argmax(predictions)]
    confidence = np.max(predictions)

    return {predicted_class: float(confidence)}

# Gradio interface
interface = gr.Interface(
    fn=predict,
    inputs=gr.Image(type="pil"),
    outputs=gr.Label(num_top_classes=3),
    title="Brain Tumor Classifier",
    description="Upload a brain MRI image to classify the tumor type."
)

interface.launch()

